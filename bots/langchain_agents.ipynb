{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.initialize import initialize_agent\n",
    "# import ollama\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from autogen import ConversableAgent, UserProxyAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# llm = ollama.GenerateResponse(model=\"llama3.2\", base_url=\"http://localhost:11434/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"base_url\": \"http://localhost:11434/v1\",\n",
    "        'api_key': 'ollama',\n",
    "    },\n",
    "]\n",
    "\n",
    "llm_config={\"config_list\": config_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import things that are needed generically\n",
    "# from langchain.pydantic_v1 import BaseModel, Field\n",
    "# from langchain.tools import BaseTool, StructuredTool, tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2\", base_url=\"http://localhost:11434/\")\n",
    "\n",
    "zero_shot_agent = ConversableAgent(\n",
    "    name=\"zero-shot-agent\",\n",
    "    system_message=\"You are helpul AI assistant\",\n",
    "    description=\"zero-shot-react-description\",\n",
    "    # tools=tools,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "userproxy = UserProxyAgent(name=\"User\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=True,\n",
    "    code_execution_config=False,\n",
    "    human_input_mode=\"ALWAYS\"   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to zero-shot-agent):\n",
      "\n",
      "hi\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mzero-shot-agent\u001b[0m (to User):\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to zero-shot-agent):\n",
      "\n",
      "nothing, just talk\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mzero-shot-agent\u001b[0m (to User):\n",
      "\n",
      "I'm happy to just chat with you then.\n",
      "\n",
      "How's your day been so far? Anything exciting or interesting happen? Or would you like me to share some random fun facts and see if we can stumble upon something interesting?\n",
      "\n",
      "We could talk about music, movies, books, or even try a random word generator challenge. Let me know what sounds good to you!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "history = userproxy.initiate_chat(zero_shot_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'hi', 'role': 'assistant'}, {'content': 'Hello! How can I assist you today?', 'role': 'user'}, {'content': 'hi', 'role': 'assistant'}, {'content': \"Another hello! Is there something on your mind that you'd like to chat about, or are you just saying hello?\", 'role': 'user'}, {'content': 'I need help with maths', 'role': 'assistant'}, {'content': \"Don't worry, math can be challenging sometimes. What kind of math do you need help with? Is it:\\n\\nAlgebra\\nGeometry\\nCalculus\\nStatistics\\nSomething else?\\n\\nTell me and I'll do my best to assist you!\", 'role': 'user'}], summary=\"Don't worry, math can be challenging sometimes. What kind of math do you need help with? Is it:\\n\\nAlgebra\\nGeometry\\nCalculus\\nStatistics\\nSomething else?\\n\\nTell me and I'll do my best to assist you!\", cost={'usage_including_cached_inference': {'total_cost': 0, 'llama3.2': {'cost': 0, 'prompt_tokens': 175, 'completion_tokens': 84, 'total_tokens': 259}}, 'usage_excluding_cached_inference': {'total_cost': 0, 'llama3.2': {'cost': 0, 'prompt_tokens': 143, 'completion_tokens': 74, 'total_tokens': 217}}}, human_input=['hi', 'I need help with maths', 'exit'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 2 \"r\"s in the word \"strawberry\".'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('how many \"r\"s are there in strawberry ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
